# train_and_submit.py
import time

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix
import joblib
import seaborn as sns
import matplotlib.pyplot as plt

# ---------- å‚æ•°ï¼ˆå¯è°ƒæ•´ï¼‰ ----------
TRAIN_CSV = "train_subset.csv"
TEST_CSV = "test_kaggle_features.csv"
ID_COL = "id"  # å¦‚æœä½ çš„ id åˆ—ä¸æ˜¯ 'id'ï¼Œè¯·ä¿®æ”¹
TARGET = "satisfaction"
RANDOM_STATE = 42
CV_FOLDS = 5
# ------------------------------------

def load_data():
    train = pd.read_csv(TRAIN_CSV)
    test = pd.read_csv(TEST_CSV)
    return train, test

# æ’é™¤ id(åœ¨é¡¹ç›®ä¸­æ²¡æœ‰å®é™…æ„ä¹‰)å’Œtarget(è¿™æ˜¯ç»“æœ )åå¯¹æ•°æ®è¿›è¡Œåˆ†ç±» numeric_cols æ•°å­—ç±»å‹, categorical_cols éæ•°å­—ç±»å‹, å­—ç¬¦ä¸²ä¹‹ç±»çš„
def infer_feature_types(df, id_col, target_col=None):
    exclude = {id_col}
    if target_col:
        exclude.add(target_col)
    # éå†æ•´ä¸ªè¡¨æ ¼, æŠŠæ’é™¤idå’Œtargetçš„åˆ—éƒ½å–å‡ºæ¥, è¿™ä¸¤ä¸ªå€¼ä¸éœ€è¦å‚ä¸è®­ç»ƒ, idå·æ— æ„ä¹‰, targetæ˜¯ç»“æœ
    cols = [c for c in df.columns if c not in exclude]
    # å–å‡ºåˆ—è¡¨ä¸­æ‰€æœ‰å±æ€§ä¸ºæ•°å­—çš„åˆ—, æ¯”å¦‚ 123, 12.3 è¿™æ ·çš„, é€šè¿‡ np.number é™å®š
    numeric_cols = df[cols].select_dtypes(include=[np.number]).columns.tolist()
    # print("æ•°å­—\n", numeric_cols)
    # å–å‡ºä¸æ˜¯æ•°å­—ç±»å‹çš„åˆ—
    categorical_cols = [c for c in cols if c not in numeric_cols]
    # print("éæ•°å­—\n", categorical_cols)
    return numeric_cols, categorical_cols

# ä½¿ç”¨ Pipeline å°è£…å·¥ä½œæµæ°´çº¿
def build_preprocessor(numeric_cols, categorical_cols):
    # æ•°å€¼åˆ—ï¼šå…ˆç”¨ä¸­ä½æ•°å¡«å……ï¼Œå†æ ‡å‡†åŒ–, è¿™é‡Œæ˜¯ä¸ª"æµæ°´çº¿"æŒ‰ç…§ä½ ç»™å®šçš„æ­¥éª¤æ‰§è¡Œ, è¿™é‡Œä¼šå°è£…ä¸¤ä¸ªæ­¥éª¤
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")), # å¡«å……ç¼ºå¤±å€¼, æ­£å¸¸è€å¸ˆç»™çš„æ•°æ®éƒ½æ²¡é—®é¢˜, è¿™ä¸€è¡ŒåŸºæœ¬æ²¡ç”¨,è¿™ä¸ªä¿è¯å¦‚æœæ•°æ®æ— æ•ˆä½¿ç”¨ä¸­é—´å€¼å¡«å……
        ("scaler", StandardScaler())
    ])
    # ç±»åˆ«åˆ—ï¼šç¼ºå¤±å¡«å……ä¸ºå­—ç¬¦ä¸² 'missing'ï¼Œå† one-hotï¼ˆdrop='if_binary' optionalï¼‰
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="constant", fill_value="missing")), # å’Œä¸Šé¢å·®ä¸å¤š
        # å°†å­—ç¬¦ä¸²è½¬ä¸ºæ•°å­—æ˜ å°„è¡¨, handle_unknown ç”¨äºæ§åˆ¶å½“é¢„æµ‹é›†ä¸­å‡ºç°è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨çš„æ•°æ®å°±å¿½ç•¥, sparse_output æ§åˆ¶æ˜¯å¦ç”¨æ´—æ¼±çŸ©é˜µè¾“å‡ºæ•°æ®, è¿™é‡Œfalseè¡¨ç¤ºè¿”å›numpyæ•°ç»„æ–¹ä¾¿æŸ¥çœ‹
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
    ])

    # å°†æ­¥éª¤ç»„åˆè½¬ä¸ºçº¯æ•°å­—çŸ©é˜µæ–¹ä¾¿è®­ç»ƒ, è¿™é‡Œå°† Pipeline å°è£…çš„æµæ°´çº¿ä¸æ•°å­—åˆ—å’Œåˆ†ç±»åˆ—å’Œå¯¹åº”, åé¢è®­ç»ƒæ—¶ä½¿ç”¨
    preprocessor = ColumnTransformer(transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols)
    ])
    return preprocessor

def baseline_models():
    # è¿”å›ä¸€ä¸ªå­—å…¸, å…¶ä¸­å®šä¹‰äº†æ¨¡å‹çš„åŸºæœ¬å‚æ•°, è€å¸ˆè¦æ±‚çš„ç®—æ³•åœ¨scikit-learnä¸­å¯¹åº”çš„æ–¹æ³•, å…·ä½“æ¨¡å‹åŸç†å’Œå®ç°æ–¹æ³•å¯ä»¥é—®ai
    # | æ¨¡å‹ | ç±»åˆ« | ç‰¹ç‚¹ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
    # | -------------------------------- | ----- | --------- | ---------- | ---------- |
    # | ** LogisticRegression ** | çº¿æ€§æ¨¡å‹ | çº¿æ€§å†³ç­–è¾¹ç•Œ | ç®€å•å¿«é€Ÿï¼Œå¯è§£é‡Šæ€§å¼º | å¤„ç†å¤æ‚å…³ç³»èƒ½åŠ›å¼± |
    # | ** KNeighborsClassifier(KNN) ** | åŸºäºè·ç¦» | çœ‹â€œé‚»å±…â€å¤šæ•°ç±»åˆ« | æ— éœ€è®­ç»ƒï¼Œæ¦‚å¿µç›´è§‚ | å¤§æ•°æ®æ…¢ï¼Œå¯¹å™ªå£°æ•æ„Ÿ |
    # | ** DecisionTreeClassifier ** | æ ‘æ¨¡å‹ | è§„åˆ™åˆ†å‰²ç‰¹å¾ç©ºé—´ | å¯è§£é‡Šï¼Œéçº¿æ€§ | å®¹æ˜“è¿‡æ‹Ÿåˆ |
    # | ** RandomForestClassifier ** | é›†æˆæ ‘æ¨¡å‹ | å¤šæ£µæ ‘æŠ•ç¥¨ | æ³›åŒ–èƒ½åŠ›å¼ºï¼Œé²æ£’æ€§å¥½ | è®­ç»ƒè¾ƒæ…¢ |
    # | ** SVC(Support Vector Machine) ** | æ ¸æ–¹æ³• | æ‰¾æœ€ä¼˜åˆ†ç•Œé¢ | å¯¹é«˜ç»´æ•°æ®å¼ºå¤§ | å‚æ•°æ•æ„Ÿï¼Œæ…¢ |
    # | ** GaussianNB(æœ´ç´ è´å¶æ–¯) ** | æ¦‚ç‡æ¨¡å‹ | ç‹¬ç«‹ç‰¹å¾å‡è®¾ | å¿«é€Ÿï¼Œé€‚åˆæ–‡æœ¬åˆ†ç±» | ç²¾åº¦è¾ƒä½ï¼Œå‡è®¾è¿‡å¼º |
    return {
        "LogisticRegression": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE), # è¦æ±‚çš„ é€»è¾‘å›å½’ ç®—æ³•
        "KNeighbors": KNeighborsClassifier(),   # è¦æ±‚çš„ KNN
        "DecisionTree": DecisionTreeClassifier(random_state=RANDOM_STATE),  # è¦æ±‚çš„ å†³ç­–æ ‘
        "RandomForest": RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1), # è¦æ±‚çš„ éšæœºæ£®æ—
        # "SVC": SVC(probability=True, random_state=RANDOM_STATE),    # è¦æ±‚çš„ æ”¯æŒå‘é‡æœº, ä¸ç”¨è¿™ä¸ª, ç®—çš„å¾ˆæ…¢
        "GaussianNB": GaussianNB()  # è¦æ±‚çš„ æœ´ç´ è´å¶æ–¯
    }

def encode_target(y_series):
    # ä¿ç•™åŸå­—ç¬¦ä¸²æ ‡ç­¾ï¼ŒåŒæ—¶æŠŠå®ƒç¼–ç ä¸º 0..K-1 ä»¥ä¾¿ sklearn ä½¿ç”¨, æ¯”å¦‚ "neutral or dissatisfied" "satisfied" ç¼–ç ä¸º 0 å’Œ 1 è¿™æ ·æ–¹ä¾¿ä½¿ç”¨
    # è¿”å› (y_encoded, label_encoder_dict)
    labels = y_series.astype(str).unique().tolist() # å–å‡º satisfaction è¿™ä¸€åˆ—å­—ç¬¦ä¸²ä¸­çš„å”¯ä¸€å€¼å˜æˆåˆ—è¡¨
    labels_sorted = sorted(labels)  # å¯¹ä¸Šé¢çš„ç»“æœåšæ’åº , æŒ‰ç…§å­—æ¯è¡¨é¡ºåº
    label_to_int = {lab: i for i, lab in enumerate(labels_sorted)} # å˜æˆå­—å…¸å³: {'neutral or dissatisfied': 0, 'satisfied': 1]
    # è§£å¼€æ³¨é‡Šå°±å¯ä»¥çœ‹åˆ°å¯¹åº”çš„å€¼, label_to_intæ˜¯ä¸ªå­—å…¸
    # print("\nlabel_to_int:", label_to_int)
    y_enc = y_series.astype(str).map(label_to_int) # å°†åŸå§‹çš„åˆ—ä¸­å­—ç¬¦ä¸²æ”¹ä¸ºæ•´æ•°å€¼, å³ä¸Šé¢ label_to_int,ä¸ºäº†æ–¹ä¾¿è®¡ç®—
    # æ‰“å°å‰10ä¸ªæ•°æ®çœ‹çœ‹è½¬åŒ–çš„å¯¹ä¸å¯¹
    # print("\ny_enc: ", y_enc.head(10))
    return y_enc.values, label_to_int

def inverse_label_map(int_preds, label_map):
    # label_map: {label_str: int}, invert
    inv = {v: k for k, v in label_map.items()}
    return [inv[int(p)] for p in int_preds]

def main():
    print("Loading data...")
    train, test = load_data()
    print(f"Train shape: {train.shape}, Test shape: {test.shape}")

    if TARGET not in train.columns:
        raise ValueError(f"Target column '{TARGET}' not found in training data.")

    # åŸºæœ¬åˆ—ç±»å‹åˆ¤æ–­, è¿™é‡Œä¸ä¼šä¿®æ”¹åŸå§‹çš„è®­ç»ƒé›†
    numeric_cols, categorical_cols = infer_feature_types(train, ID_COL, TARGET)
    print("\nNumeric cols:", numeric_cols)
    print("\nCategorical cols:", categorical_cols)

    # ç›®æ ‡ç¼–ç , æŒ‰ç…§éœ€æ±‚å°±æ˜¯ satisfaction è¿™ä¸€åˆ—æ•°æ®
    y_raw = train[TARGET]
    train_target, label_map = encode_target(y_raw)
    print("\nLabel mapping:", label_map)
    # print("\nTrain_target:", train_target)
    # ç‰¹å¾çŸ©é˜µ, è¿™é‡Œå»æ‰idåˆ—å’Œsatisfactionåˆ—, å®é™…è®­ç»ƒçš„æ—¶å€™åªæœ‰ç‰¹å¾æ•°æ®æœ‰ç”¨
    train_drop = train.drop(columns=[ID_COL, TARGET])
    # å¦‚æœæµ‹è¯•é›†ä¹Ÿæœ‰idçš„è¯ä¹Ÿåˆ é™¤
    if ID_COL in test.columns:
        x_test = test.drop(columns=[ID_COL])
    else:
        x_test = test.copy()

    # å»ºé¢„å¤„ç†å™¨
    preprocessor = build_preprocessor(numeric_cols, categorical_cols)

    # æ‹†åˆ†éªŒè¯é›†, æŒ‰ç…§ test_size å€¼æ‹†åˆ†, è¿™é‡Œçš„åŠŸèƒ½ç”¨äºéªŒè¯è€å¸ˆè¦æ±‚çš„æ¨¡å‹, å…ˆæ‹†åˆ†è®­ç»ƒé›†ç„¶åç”¨baseline_models()é‡Œçš„æ¨¡å‹è®¡ç®—è®­ç»ƒé›†æ•°æ®ä»è€Œæ‹¿åˆ°æœ€é€‚åˆçš„æ¨¡å‹ç”¨äºåç»­é¢„æµ‹
    # | å˜é‡å | å«ä¹‰ |
    # | --------- | --------------- |
    # | `x_train` | ç”¨äºè®­ç»ƒæ¨¡å‹çš„è¾“å…¥æ•°æ® |
    # | `x_val` | ç”¨äºéªŒè¯ï¼ˆæµ‹è¯•ï¼‰æ¨¡å‹çš„è¾“å…¥æ•°æ® |
    # | `y_train` | è®­ç»ƒé›†å¯¹åº”çš„æ ‡ç­¾ |
    # | `y_val` | éªŒè¯é›†å¯¹åº”çš„æ ‡ç­¾ |
    x_train, x_val, y_train, y_val = train_test_split(train_drop, # è®­ç»ƒé›†, å‰é¢å»é™¤äº† id å’Œ targetçš„ç»“æœ;
                                                      train_target, # ç›®æ ‡, ä¹Ÿå°±æ˜¯ satisfaction, train_drop å’Œ yä¸€ä¸€å¯¹åº”;
                                                      test_size=0.2, # è¡¨ç¤ºéªŒè¯é›†å å…¨éƒ¨æ•°æ®çš„ 20%, è®­ç»ƒé›†å  80%.
                                                      stratify=train_target, # æ‹†åˆ†æ•°æ®, åœ¨æ‹†åˆ†æ•°æ®æ—¶, æŒ‰ç…§æ ‡ç­¾ train_target çš„åˆ†å¸ƒæ¯”ä¾‹æ¥åˆ†å‰²æ•°æ®, å¦‚æœä¸è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´éªŒè¯é›†æ¯”ä¾‹å¤±è¡¡
                                                      # è®¾ç½®éšæœºå€¼, ç²—æµ…å¯ä»¥ç†è§£ä¸º train_test_split() åœ¨éšæœºæŠ½å–æ ·æœ¬æ—¶çš„è¡Œä¸ºæ˜¯éšæœºçš„,
                                                      # è¿™é‡Œè®¾ç½®å›ºå®šå€¼, è®©ä»–è¡Œä¸ºä¸éšæœºä¿è¯åœ¨ä¸åŒç”µè„‘éƒ½èƒ½å¾—åˆ°ç›¸åŒç»“æœ
                                                      random_state=RANDOM_STATE)
    print(f"\nTrain/Val shapes: {x_train.shape}/{x_val.shape}")

    # å®šä¹‰å€™é€‰æ¨¡å‹
    models = baseline_models()

    # ç”¨å€™é€‰æ¨¡å‹åˆ†åˆ«è®¡ç®—è®­ç»ƒé›†, çœ‹çœ‹é‚£ä¸ªæ¨¡å‹æœ€å¥½, å…¶å®å¯ä»¥ä¸ç”¨è¿™ä¹ˆåš éšä¾¿é€‰ä¸€ä¸ªä¹Ÿè¡Œ, ä½†æ˜¯ä¸åŒæ¨¡å‹é€‚åˆçš„æ•°æ®åœºæ™¯æœ‰ä¼˜åŠ£, é€‰å‡ºæœ€å¥½çš„æ¨¡å‹å¯ä»¥å¾—åˆ°æœ€å¥½çš„æµ‹è¯•ç»“æœ
    # è¿™é‡ŒæŒ‰ç…§è€å¸ˆçš„è¦æ±‚æµ‹è¯•ä¸åŒç®—æ³•çš„å‡†ç¡®ç‡, F1-score, æ··æ·†çŸ©é˜µ
    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    model_scores = {} # æ¯æ¬¡è®¡ç®—çš„ç»“æœä¿å­˜åœ¨é‡Œé¢, åé¢æ¯”è¾ƒåˆ†æ•°æœ€é«˜åœ°ç”¨äºæœ¬æ¬¡è®­ç»ƒå’ŒéªŒè¯
    for name, clf in models.items():
        print("=" * 60)
        print(f"ğŸ” äº¤å‰éªŒè¯è¯„ä¼° {name} ...")

        # æ„å»º pipelineï¼ˆé¢„å¤„ç† + æ¨¡å‹ï¼‰, preprocessor æ˜¯ build_preprocessor()
        # çš„ç»“æœåŒ…å«äº†è¡¨æ ¼ä¸­ä¸åŒåˆ—æ‰§è¡Œçš„æµç¨‹ categorical_transformer å’Œ numeric_transformer
        pipe = Pipeline(steps=[
            ("preproc", preprocessor),
            ("clf", clf)
        ])

        # ----- 1ï¸âƒ£ F1-score äº¤å‰éªŒè¯ -----
        f1_scores = cross_val_score(pipe, train_drop, train_target, cv=cv, scoring="f1_macro", n_jobs=-1)
        acc_scores = cross_val_score(pipe, train_drop, train_target, cv=cv, scoring="accuracy", n_jobs=-1)

        f1_mean, f1_std = f1_scores.mean(), f1_scores.std()
        acc_mean, acc_std = acc_scores.mean(), acc_scores.std()

        model_scores[name] = {
            "F1_mean": f1_mean,
            "F1_std": f1_std,
            "ACC_mean": acc_mean,
            "ACC_std": acc_std
        }

        print(f"{name}:")
        print(f"  âœ… mean F1_macro = {f1_mean:.4f} (+/- {f1_std:.4f})")
        print(f"  âœ… mean Accuracy = {acc_mean:.4f} (+/- {acc_std:.4f})")

        # ----- 2ï¸âƒ£ åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒå¹¶ç”Ÿæˆæ··æ·†çŸ©é˜µ -----
        pipe.fit(x_train, y_train) # è®­ç»ƒ
        y_pred = pipe.predict(x_val) # é¢„æµ‹

        print("\nğŸ“Š æ··æ·†çŸ©é˜µï¼š")
        cm = confusion_matrix(y_val, y_pred)
        print(cm)

        # æ‰“å°è¯¦ç»†æŠ¥å‘Šï¼ˆPrecision, Recall, F1ï¼‰
        print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Šï¼š")
        print(classification_report(y_val, y_pred, digits=4))

        # ç»˜åˆ¶æ··æ·†çŸ©é˜µå›¾, ç”Ÿæˆè§†å›¾ å¯ä»¥ä¸ä½¿ç”¨
        # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        # plt.title(f"Confusion Matrix - {name}")
        # plt.xlabel("Predicted")
        # plt.ylabel("Actual")
        # plt.show()

    # 3ï¸âƒ£ è¾“å‡ºæ€»ç»“æœæ±‡æ€»è¡¨
    print("\n\nğŸ å„æ¨¡å‹äº¤å‰éªŒè¯å¹³å‡åˆ†å¯¹æ¯”ï¼š")
    for name, score in model_scores.items():
        print(f"{name:20s} | F1={score['F1_mean']:.4f} | ACC={score['ACC_mean']:.4f}")


    # é€‰æ‹©åœ¨ CV ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼ˆæŒ‰ mean F1_macroï¼‰
    best_model_name = max(model_scores.items(), key=lambda kv: kv[1]['F1_mean'])[0]
    print("Best model by CV F1_macro:", best_model_name)

    # è¿™é‡Œå¯¹æ¨¡å‹è¿›è¡Œè°ƒå‚æµ‹è¯•, ç”±äºå‰é¢çš„ä»£ç å·²ç»å¯ä»¥çŸ¥é“é’ˆå¯¹è¿™éƒ¨åˆ†æ•°æ® RandomForest æ¨¡å‹çš„åˆ†æ•°æœ€å¥½
    # è¿™é‡Œé’ˆå¯¹å®ƒè¿›è¡Œä¼˜åŒ–è°ƒå‚, ä½¿ç”¨ GridSearchCV è¿›è¡Œè‡ªåŠ¨é…ç½®æ¨¡å‹å‚æ•°, ä½†æ˜¯ GridSearchCV éœ€è¦å…ˆæ‰‹åŠ¨é…ç½®ä¸€ä¸‹å‚æ•°,
    # è¿™éƒ¨åˆ†ä»£ç ä¹‹æ‰€ä»¥å­˜åœ¨æ˜¯å› ä¸ºè€å¸ˆè¦æ±‚çš„, ç›´æ¥èµ° elseçš„æµç¨‹ä¹Ÿè¡Œ, åªä¸è¿‡ä½¿ç”¨çš„æ˜¯ RandomForest é»˜è®¤å‚æ•°,
    # é»˜è®¤å‚æ•°çš„å¾—åˆ†å¯èƒ½æ²¡æœ‰ä¼˜åŒ–åçš„é«˜.
    if "RandomForest" in models:
        print("Running GridSearchCV on RandomForest (example)...")
        rf_pipe = Pipeline(steps=[("preproc", preprocessor), ("clf", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))])
        # é…ç½® GridSearchCV çš„å‚æ•°
        param_grid = {
            "clf__n_estimators": [100, 200],
            "clf__max_depth": [None, 10, 20],
            "clf__min_samples_split": [2, 5]
        }
        gs = GridSearchCV(rf_pipe, param_grid, cv=cv, scoring="f1_macro", n_jobs=-1, verbose=1)
        gs.fit(x_train, y_train)
        print("GridSearch best params:", gs.best_params_)
        print("GridSearch best score (F1_macro):", gs.best_score_)
        # å¦‚æœ GridSearch çš„æœ€ä½³æ¯”ä¹‹å‰æœ€å¥½çš„è¿˜å¥½ï¼Œå°±é€‰å®ƒ
        if gs.best_score_ > model_scores[best_model_name]['F1_mean']:
            print("GridSearch RandomForest beats previous best. Selecting GridSearch RF as final model.")
            final_pipeline = gs.best_estimator_
            final_name = "RandomForest_GridSearch"
        else:
            # å¦åˆ™ç”¨ä¹‹å‰é€‰å‡ºçš„
            final_pipeline = Pipeline(steps=[("preproc", preprocessor), ("clf", models[best_model_name])])
            final_pipeline.fit(x_train, y_train)
            final_name = best_model_name
    else:
        final_pipeline = Pipeline(steps=[("preproc", preprocessor), ("clf", models[best_model_name])])
        final_pipeline.fit(x_train, y_train)
        final_name = best_model_name

    print("Final model selected:", final_name)

    # åœ¨ä¿ç•™çš„éªŒè¯é›†ä¸Šè¯„ä¼°
    print("Evaluating on held-out validation set...")
    y_val_pred = final_pipeline.predict(x_val)
    acc = accuracy_score(y_val, y_val_pred)
    f1 = f1_score(y_val, y_val_pred, average="macro")
    print(f"Validation Accuracy: {acc:.4f}, F1_macro: {f1:.4f}")
    print("Classification report (validation):")
    # æŠŠé¢„æµ‹ int è½¬å›åŸå§‹æ ‡ç­¾å­—ç¬¦ä¸²ä»¥ä¾¿å¯è¯»æ€§
    y_val_pred_labels = inverse_label_map(y_val_pred, label_map)
    y_val_labels = inverse_label_map(y_val, label_map)
    print(classification_report(y_val_labels, y_val_pred_labels))

    # å¯¹æµ‹è¯•é›†åšé¢„æµ‹
    print("Predicting on test set...")
    test_pred_int = final_pipeline.predict(x_test)
    test_pred_labels = inverse_label_map(test_pred_int, label_map)

    # ç”Ÿæˆ submission, ç”¨ pd åº“ç”Ÿæˆcsvåˆ—è¡¨æ–‡ä»¶
    submission = pd.DataFrame({
        "ID": test[ID_COL] if ID_COL in test.columns else np.arange(len(test_pred_labels)),
        "label": test_pred_labels
    })
    submission.to_csv("my_submission.csv", index=False)
    print("Saved my_submission.csv")

    # ä¿å­˜æ¨¡å‹ä»¥å¤‡ç­”è¾©/å¤ç°
    joblib.dump(final_pipeline, "final_model_pipeline.joblib")
    print("Saved final_model_pipeline.joblib")

if __name__ == "__main__":
    print("run time:", time.time())
    main()

